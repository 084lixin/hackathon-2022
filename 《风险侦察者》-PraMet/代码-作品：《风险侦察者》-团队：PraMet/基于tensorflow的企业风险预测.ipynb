{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "#编写TextCNN类\n",
    "class TextCNN:\n",
    "    def __init__(self, filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,\n",
    "                 is_training,initializer=tf.random_normal_initializer(stddev=0.1),multi_label_flag=False,clip_gradients=5.0,decay_rate_big=0.50):\n",
    "        \"\"\"init all hyperparameter here\"\"\"\n",
    "        # 设置参数\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length=sequence_length\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size=embed_size\n",
    "        self.is_training=is_training\n",
    "        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=\"learning_rate\")#ADD learning_rate\n",
    "        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * decay_rate_big)\n",
    "        self.filter_sizes=filter_sizes # it is a list of int. e.g. [3,4,5]\n",
    "        self.num_filters=num_filters\n",
    "        self.initializer=initializer\n",
    "        self.num_filters_total=self.num_filters * len(filter_sizes) #how many filters totally.\n",
    "        self.multi_label_flag=multi_label_flag\n",
    "        self.clip_gradients = clip_gradients\n",
    " \n",
    "        # 设置标签参数\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")  # X\n",
    "        self.input_y = tf.placeholder(tf.int32, [None,],name=\"input_y\")  # y:[None,num_classes]\n",
    "        self.input_y_multilabel = tf.placeholder(tf.float32,[None,self.num_classes], name=\"input_y_multilabel\")  # y:[None,num_classes]. this is for multi-label classification only.\n",
    "        self.dropout_keep_prob=tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n",
    " \n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
    "        self.epoch_step=tf.Variable(0,trainable=False,name=\"Epoch_Step\")\n",
    "        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    " \n",
    "        self.instantiate_weights()\n",
    "        self.logits = self.inference()\n",
    "        if not is_training:\n",
    "            return\n",
    "        if multi_label_flag:\n",
    "            print(\"going to use multi label loss.\")\n",
    "            self.loss_val = self.loss_multilabel()\n",
    "        else:\n",
    "            print(\"going to use single label loss.\")\n",
    "            self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")  # shape:[None,]\n",
    " \n",
    "        if not self.multi_label_flag:\n",
    "            correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n",
    "            self.accuracy =tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\") # shape=()\n",
    "        else:\n",
    "            self.accuracy = tf.constant(0.5) #fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\n",
    " \n",
    "     #设置权重\n",
    "    def instantiate_weights(self):\n",
    "        \"\"\"define all weights here\"\"\"\n",
    "        with tf.name_scope(\"embedding\"): # embedding matrix\n",
    "            self.Embedding = tf.get_variable(\"Embedding\",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) \n",
    "            self.W_projection = tf.get_variable(\"W_projection\",shape=[self.num_filters_total, self.num_classes],initializer=self.initializer) \n",
    "            self.b_projection = tf.get_variable(\"b_projection\",shape=[self.num_classes])       \n",
    " \n",
    "    def inference(self):\n",
    "        \"\"\"main computation graph here: 1.embedding-->2.average-->3.linear classifier\"\"\"\n",
    "        # 获得文本长度等信息\n",
    "        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x)\n",
    "        self.sentence_embeddings_expanded=tf.expand_dims(self.embedded_words,-1) \n",
    " \n",
    "        pooled_outputs = []\n",
    "        for i,filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"convolution-pooling-%s\" %filter_size):\n",
    "               #过滤器设置\n",
    "                filter = tf.get_variable(\"filter-%s\"%filter_size,[filter_size,self.embed_size,1,self.num_filters],initializer=self.initializer)\n",
    "                conv=tf.nn.conv2d(self.sentence_embeddings_expanded, filter, strides=[1,1,1,1], padding=\"VALID\",name=\"conv\") \n",
    "                b=tf.get_variable(\"b-%s\"%filter_size,[self.num_filters]) \n",
    "                h=tf.nn.relu(tf.nn.bias_add(conv,b),\"relu\") \n",
    "            \n",
    "                pooled = tf.nn.max_pool(h, ksize=[1,self.sequence_length-filter_size+1,1,1], strides=[1,1,1,1], padding='VALID',name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "        self.h_pool=tf.concat(pooled_outputs,3) \n",
    "        self.h_pool_flat=tf.reshape(self.h_pool,[-1,self.num_filters_total]) \n",
    " \n",
    "        #过滤层\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop  =  tf.nn.dropout(self.h_pool_flat,keep_prob=self.dropout_keep_prob) \n",
    "        with tf.name_scope(\"output\"):\n",
    "            logits = tf.matmul(self.h_drop,self.W_projection) + self.b_projection \n",
    "        return logits\n",
    " #损失函数\n",
    "    def loss(self,l2_lambda=0.0001):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);\n",
    "            loss=tf.reduce_mean(losses)\n",
    "            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "            loss=loss+l2_losses\n",
    "        return loss\n",
    " \n",
    "    def loss_multilabel(self,l2_lambda=0.00001): \n",
    "        with tf.name_scope(\"loss\"):\n",
    "          \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel, logits=self.logits);\n",
    "            print(\"sigmoid_cross_entropy_with_logits.losses:\",losses) \n",
    "            losses=tf.reduce_sum(losses,axis=1)\n",
    "            loss=tf.reduce_mean(losses)         \n",
    "            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "            loss=loss+l2_losses\n",
    "        return loss\n",
    " #训练集处理\n",
    "    def train(self):\n",
    "        \"\"\"based on the loss, use SGD to update parameter\"\"\"\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=\"Adam\",clip_gradients=self.clip_gradients)\n",
    "        return train_op\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "MAX_FEATURES = 1500\n",
    "MAX_SENTENCE_LENGTH = 100\n",
    " \n",
    "#设置初始参数\n",
    "lr = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 127\n",
    "vocab_size = MAX_FEATURES\n",
    "embedding_size = 100\n",
    "n_inputs = embedding_size  # 最小数据输入维度\n",
    "n_steps = MAX_SENTENCE_LENGTH  # 步长\n",
    "n_hidden_units = 128  \n",
    " \n",
    "def get_sentiment_data():\n",
    "    df_sentiment = pd.read_csv('sentiment.csv', encoding='utf-8')\n",
    "    sentenses = df_sentiment['sentence'].values\n",
    "    sentenses = [s.lower() for s in sentenses]\n",
    "    wordlist_sentence = [nltk.word_tokenize(s) for s in sentenses]\n",
    "    ws = []\n",
    "    for wordlist in wordlist_sentence:\n",
    "        ws.extend(wordlist)\n",
    "    word_counter = Counter(ws)\n",
    "    mc = word_counter.most_common(100)\n",
    "\n",
    "    vocab_size = min(MAX_FEATURES, len(word_counter)) + 2\n",
    "    word2index = {x[0]: i + 2 for i, x in\n",
    "                  enumerate(word_counter.most_common(MAX_FEATURES))}\n",
    "    word2index[\"PAD\"] = 0\n",
    "    word2index[\"UNK\"] = 1\n",
    "    index2word = {v: k for k, v in word2index.items()}\n",
    "    res = []\n",
    "    print('iterrows')\n",
    "    for line in df_sentiment.iterrows():\n",
    "        label, sentence = str(line[1]['label']), line[1]['sentence']\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        for word in words:\n",
    "            if word in word2index.keys():\n",
    "                seqs1.append(word2index[word])\n",
    "            else:\n",
    "                seqs1.append(word2index[\"UNK\"])\n",
    "        if MAX_SENTENCE_LENGTH < len(seqs1):\n",
    "            print('unexpected length of padding', len(padding), padding)\n",
    "            continue\n",
    "        padding = [0] * (MAX_SENTENCE_LENGTH - len(seqs1))\n",
    "        padding.extend(seqs1)\n",
    "        if len(padding) != MAX_SENTENCE_LENGTH:\n",
    "            print('unexpected length of padding', len(padding), padding)\n",
    "        if label == '0':\n",
    "            res.append([1, padding])\n",
    "        if label == '1':\n",
    "            res.append([0, padding])\n",
    "    return res,vocab_size\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test函数\n",
    "def test():\n",
    "    num_classes=2\n",
    "    learning_rate=lr\n",
    "    batch_size=100\n",
    "    decay_steps=1000\n",
    "    decay_rate=0.9\n",
    "\n",
    "    is_training=True\n",
    "    dropout_keep_prob=1 \n",
    "    filter_sizes=[3,4]\n",
    "    num_filters=128\n",
    " \n",
    "    sentiment,vocab_size = get_sentiment_data()\n",
    "    input_x = [x for y,x in sentiment]\n",
    "    Ys = [y for y,x in sentiment]\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(input_x, Ys, test_size=0.1, random_state=42, stratify=Ys)\n",
    " \n",
    "    textRNN=TextCNN(filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,MAX_SENTENCE_LENGTH,vocab_size,embedding_size,is_training)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(100):\n",
    "            print('\\n--------TRAIN--------:  No.', (i ) * batch_size)\n",
    "            if (i+1)*batch_size >len(Xtrain):\n",
    "                print('test No.',(i+1)*batch_size )\n",
    "                break\n",
    "            input_x = Xtrain[i*batch_size:(i+1)*batch_size]\n",
    "            input_y = ytrain[i*batch_size:(i+1)*batch_size]\n",
    "            loss,acc,predict,W_projection_value,_=sess.run(\n",
    "                [textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.W_projection,textRNN.train_op],\n",
    "                feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n",
    "            print(\"loss:\",loss,\"acc:\",acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0453a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38cfa62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
